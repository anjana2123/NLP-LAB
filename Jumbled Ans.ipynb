{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "83724eec-180f-4ed6-b7a5-6aaa9fae9da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.stem import PorterStemmer\n",
    "import pandas as pd\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4f03cbe1-e881-403a-9463-2f6d2b9c7487",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(r\"C:\\Users\\anjan\\Desktop\\NER_Dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3ac2ca3e-5f54-44ed-882c-55bab6aa3f7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence_ID</th>\n",
       "      <th>Word</th>\n",
       "      <th>POS</th>\n",
       "      <th>Tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>['Thousands', 'of', 'demonstrators', 'have', '...</td>\n",
       "      <td>['NNS', 'IN', 'NNS', 'VBP', 'VBN', 'IN', 'NNP'...</td>\n",
       "      <td>['O', 'O', 'O', 'O', 'O', 'O', 'B-geo', 'O', '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sentence: 10</td>\n",
       "      <td>['Iranian', 'officials', 'say', 'they', 'expec...</td>\n",
       "      <td>['JJ', 'NNS', 'VBP', 'PRP', 'VBP', 'TO', 'VB',...</td>\n",
       "      <td>['B-gpe', 'O', 'O', 'O', 'O', 'O', 'O', 'O', '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sentence: 100</td>\n",
       "      <td>['Helicopter', 'gunships', 'Saturday', 'pounde...</td>\n",
       "      <td>['NN', 'NNS', 'NNP', 'VBD', 'JJ', 'NNS', 'IN',...</td>\n",
       "      <td>['O', 'O', 'B-tim', 'O', 'O', 'O', 'O', 'O', '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sentence: 1000</td>\n",
       "      <td>['They', 'left', 'after', 'a', 'tense', 'hour-...</td>\n",
       "      <td>['PRP', 'VBD', 'IN', 'DT', 'NN', 'JJ', 'NN', '...</td>\n",
       "      <td>['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sentence: 10000</td>\n",
       "      <td>['U.N.', 'relief', 'coordinator', 'Jan', 'Egel...</td>\n",
       "      <td>['NNP', 'NN', 'NN', 'NNP', 'NNP', 'VBD', 'NNP'...</td>\n",
       "      <td>['B-geo', 'O', 'O', 'B-per', 'I-per', 'O', 'B-...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Sentence_ID                                               Word  \\\n",
       "0      Sentence: 1  ['Thousands', 'of', 'demonstrators', 'have', '...   \n",
       "1     Sentence: 10  ['Iranian', 'officials', 'say', 'they', 'expec...   \n",
       "2    Sentence: 100  ['Helicopter', 'gunships', 'Saturday', 'pounde...   \n",
       "3   Sentence: 1000  ['They', 'left', 'after', 'a', 'tense', 'hour-...   \n",
       "4  Sentence: 10000  ['U.N.', 'relief', 'coordinator', 'Jan', 'Egel...   \n",
       "\n",
       "                                                 POS  \\\n",
       "0  ['NNS', 'IN', 'NNS', 'VBP', 'VBN', 'IN', 'NNP'...   \n",
       "1  ['JJ', 'NNS', 'VBP', 'PRP', 'VBP', 'TO', 'VB',...   \n",
       "2  ['NN', 'NNS', 'NNP', 'VBD', 'JJ', 'NNS', 'IN',...   \n",
       "3  ['PRP', 'VBD', 'IN', 'DT', 'NN', 'JJ', 'NN', '...   \n",
       "4  ['NNP', 'NN', 'NN', 'NNP', 'NNP', 'VBD', 'NNP'...   \n",
       "\n",
       "                                                 Tag  \n",
       "0  ['O', 'O', 'O', 'O', 'O', 'O', 'B-geo', 'O', '...  \n",
       "1  ['B-gpe', 'O', 'O', 'O', 'O', 'O', 'O', 'O', '...  \n",
       "2  ['O', 'O', 'B-tim', 'O', 'O', 'O', 'O', 'O', '...  \n",
       "3  ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', ...  \n",
       "4  ['B-geo', 'O', 'O', 'B-per', 'I-per', 'O', 'B-...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c4ac010a-5d64-4fd9-aa7b-f7ba61007980",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'sentence'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3791\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3790\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3791\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   3792\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mindex.pyx:152\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mindex.pyx:181\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:7080\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:7088\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'sentence'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m tags \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, row \u001b[38;5;129;01min\u001b[39;00m df\u001b[38;5;241m.\u001b[39miterrows():\n\u001b[1;32m----> 4\u001b[0m     sentence \u001b[38;5;241m=\u001b[39m row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msentence\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m      5\u001b[0m     sentence_tokens \u001b[38;5;241m=\u001b[39m [stemmer\u001b[38;5;241m.\u001b[39mstem(word\u001b[38;5;241m.\u001b[39mlower()) \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m word_tokenize(sentence) \u001b[38;5;28;01mif\u001b[39;00m word\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m stop_words]\n\u001b[0;32m      6\u001b[0m     sentences\u001b[38;5;241m.\u001b[39mappend(sentence_tokens)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\series.py:1040\u001b[0m, in \u001b[0;36mSeries.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   1037\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[key]\n\u001b[0;32m   1039\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m key_is_scalar:\n\u001b[1;32m-> 1040\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_value(key)\n\u001b[0;32m   1042\u001b[0m \u001b[38;5;66;03m# Convert generator to list before going through hashable part\u001b[39;00m\n\u001b[0;32m   1043\u001b[0m \u001b[38;5;66;03m# (We will iterate through the generator there to check for slices)\u001b[39;00m\n\u001b[0;32m   1044\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\series.py:1156\u001b[0m, in \u001b[0;36mSeries._get_value\u001b[1;34m(self, label, takeable)\u001b[0m\n\u001b[0;32m   1153\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[label]\n\u001b[0;32m   1155\u001b[0m \u001b[38;5;66;03m# Similar to Index.get_value, but we do not fall back to positional\u001b[39;00m\n\u001b[1;32m-> 1156\u001b[0m loc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex\u001b[38;5;241m.\u001b[39mget_loc(label)\n\u001b[0;32m   1158\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(loc):\n\u001b[0;32m   1159\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[loc]\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3798\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3793\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m   3794\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[0;32m   3795\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[0;32m   3796\u001b[0m     ):\n\u001b[0;32m   3797\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[1;32m-> 3798\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3799\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3800\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3801\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3802\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3803\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'sentence'"
     ]
    }
   ],
   "source": [
    "sentences = []\n",
    "tags = []\n",
    "for i, row in df.iterrows():\n",
    "    sentence = row['sentence']\n",
    "    sentence_tokens = [stemmer.stem(word.lower()) for word in word_tokenize(sentence) if word.lower() not in stop_words]\n",
    "    sentences.append(sentence_tokens)\n",
    "    tags.append([tag.strip() for tag in row['tags'].split(',')])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6b69b60-c576-4901-80e3-0544e9912cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the tag transition probabilities\n",
    "tag_transition_prob = defaultdict(lambda: defaultdict(float))\n",
    "for sentence, sentence_tags in zip(sentences, tags):\n",
    "    for i in range(len(sentence_tags) - 1):\n",
    "        tag_transition_prob[sentence_tags[i]][sentence_tags[i+1]] += 1\n",
    "\n",
    "for tag, next_tag_prob in tag_transition_prob.items():\n",
    "    total = sum(next_tag_prob.values())\n",
    "    for next_tag, count in next_tag_prob.items():\n",
    "        tag_transition_prob[tag][next_tag] /= total\n",
    "\n",
    "# Define a function to find the most probable sequence of tags for a given jumbled sentence\n",
    "def find_tag_sequence(jumbled_sentence):\n",
    "    jumbled_tokens = [stemmer.stem(word.lower()) for word in word_tokenize(jumbled_sentence) if word.lower() not in stop_words]\n",
    "    \n",
    "    # Initialize the Viterbi table and backpointer table\n",
    "    viterbi = np.zeros((len(jumbled_tokens), len(set(tag for tags in tags for tag in tags))))\n",
    "    backpointer = np.zeros((len(jumbled_tokens), len(set(tag for tags in tags for tag in tags))), dtype=int)\n",
    "    \n",
    "    # Iterate through the tokens in the jumbled sentence\n",
    "    for i, token in enumerate(jumbled_tokens):\n",
    "        # Find all possible tags for the current token\n",
    "        possible_tags = set()\n",
    "        for sentence_tags in tags:\n",
    "            for tag in sentence_tags:\n",
    "                if token in [stemmer.stem(word.lower()) for word in sentences[i]]:\n",
    "                    possible_tags.add(tag)\n",
    "        \n",
    "        # Initialize the Viterbi and backpointer tables for the first token\n",
    "        if i == 0:\n",
    "            for j, tag in enumerate(possible_tags):\n",
    "                viterbi[i, j] = tag_transition_prob['<start>'][tag]\n",
    "                backpointer[i, j] = -1\n",
    "        # Update the Viterbi and backpointer tables for subsequent tokens\n",
    "        else:\n",
    "            for j, tag in enumerate(possible_tags):\n",
    "                max_prob = 0\n",
    "                max_prev_tag_idx = -1\n",
    "                for k, prev_tag in enumerate(possible_tags):\n",
    "                    prob = viterbi[i-1, k] * tag_transition_prob[prev_tag][tag]\n",
    "                    if prob > max_prob:\n",
    "                        max_prob = prob\n",
    "                        max_prev_tag_idx = k\n",
    "                viterbi[i, j] = max_prob\n",
    "                backpointer[i, j] = max_prev_tag_idx\n",
    "    \n",
    "    # Find the most probable sequence of tags\n",
    "    most_probable_tags = []\n",
    "    current_tag_idx = np.argmax(viterbi[-1, :])\n",
    "    for i in range(len(jumbled_tokens)-1, -1, -1):\n",
    "        most_probable_tags.insert(0, list(possible_tags)[current_tag_idx])\n",
    "        current_tag_idx = backpointer[i, current_tag_idx]\n",
    "    \n",
    "    return most_probable_tags\n",
    "\n",
    "# Example usage\n",
    "jumbled_sentence = \"The quick brown fox jumps over the lazy dog.\"\n",
    "most_probable_tags = find_tag_sequence(jumbled_sentence)\n",
    "print(f\"Most probable tags for the jumbled sentence: {', '.join(most_probable_tags)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0e630584-4369-41cc-85e2-8d69f433286e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(r\"C:\\Users\\anjan\\Desktop\\NER_Dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ebfebc01-c651-49b7-9bd4-8ef9137e48ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the dataset\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "sentences = []\n",
    "pos_tags = []\n",
    "tags = []\n",
    "for _, row in df.iterrows():\n",
    "    sentence = row['Word'].split(',')\n",
    "    sentence = [stemmer.stem(word.lower()) for word in sentence if word.lower() not in stop_words]\n",
    "    pos_tag = row['POS'].split(',')\n",
    "    tag = row['Tag'].split(',')\n",
    "    sentences.append(sentence)\n",
    "    pos_tags.append(pos_tag)\n",
    "    tags.append(tag)\n",
    "    # Build the tag transition probabilities\n",
    "tag_transition_prob = defaultdict(lambda: defaultdict(float))\n",
    "for sentence_tags in tags:\n",
    "    for i in range(len(sentence_tags) - 1):\n",
    "        tag_transition_prob[sentence_tags[i]][sentence_tags[i+1]] += 1\n",
    "\n",
    "for tag, next_tag_prob in tag_transition_prob.items():\n",
    "    total = sum(next_tag_prob.values())\n",
    "    for next_tag, count in next_tag_prob.items():\n",
    "        tag_transition_prob[tag][next_tag] /= total\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e45b528d-f184-41b6-a0fb-8f9459b1df8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to find the most probable sequence of tags for a given jumbled sentence\n",
    "def find_tag_sequence(jumbled_sentence):\n",
    "    jumbled_tokens = [stemmer.stem(word.lower()) for word in word_tokenize(jumbled_sentence) if word.lower() not in stop_words]\n",
    "    \n",
    "    # Initialize the Viterbi table and backpointer table\n",
    "    viterbi = np.zeros((len(jumbled_tokens), len(set(tag for tags in tags for tag in tags))))\n",
    "    backpointer = np.zeros((len(jumbled_tokens), len(set(tag for tags in tags for tag in tags))), dtype=int)\n",
    "    \n",
    "    # Iterate through the tokens in the jumbled sentence\n",
    "    for i, token in enumerate(jumbled_tokens):\n",
    "        # Find all possible tags for the current token\n",
    "        possible_tags = set()\n",
    "        for sentence, sentence_tags in zip(sentences, tags):\n",
    "            if token in sentence:\n",
    "                for j, word in enumerate(sentence):\n",
    "                    if word == token:\n",
    "                        possible_tags.add(sentence_tags[j])\n",
    "        \n",
    "        # Initialize the Viterbi and backpointer tables for the first token\n",
    "        if i == 0:\n",
    "            for j, tag in enumerate(possible_tags):\n",
    "                viterbi[i, j] = tag_transition_prob['<start>'][tag]\n",
    "                backpointer[i, j] = -1\n",
    "        # Update the Viterbi and backpointer tables for subsequent tokens\n",
    "        else:\n",
    "            for j, tag in enumerate(possible_tags):\n",
    "                max_prob = 0\n",
    "                max_prev_tag_idx = -1\n",
    "                for k, prev_tag in enumerate(possible_tags):\n",
    "                    prob = viterbi[i-1, k] * tag_transition_prob[prev_tag][tag]\n",
    "                    if prob > max_prob:\n",
    "                        max_prob = prob\n",
    "                        max_prev_tag_idx = k\n",
    "                viterbi[i, j] = max_prob\n",
    "                backpointer[i, j] = max_prev_tag_idx\n",
    "    \n",
    "    # Find the most probable sequence of tags\n",
    "    most_probable_tags = []\n",
    "    current_tag_idx = np.argmax(viterbi[-1, :])\n",
    "    for i in range(len(jumbled_tokens)-1, -1, -1):\n",
    "        most_probable_tags.insert(0, list(possible_tags)[current_tag_idx])\n",
    "        current_tag_idx = backpointer[i, current_tag_idx]\n",
    "    \n",
    "    return most_probable_tags\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "689c991b-4d55-4693-a256-166f2ea24b04",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Example usage\u001b[39;00m\n\u001b[0;32m      2\u001b[0m jumbled_sentence \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe quick brown fox over jumps the lazy dog\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 3\u001b[0m most_probable_tags \u001b[38;5;241m=\u001b[39m find_tag_sequence(jumbled_sentence)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMost probable tags for the jumbled sentence: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(most_probable_tags)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[24], line 41\u001b[0m, in \u001b[0;36mfind_tag_sequence\u001b[1;34m(jumbled_sentence)\u001b[0m\n\u001b[0;32m     39\u001b[0m current_tag_idx \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(viterbi[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :])\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(jumbled_tokens)\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m---> 41\u001b[0m     most_probable_tags\u001b[38;5;241m.\u001b[39minsert(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mlist\u001b[39m(possible_tags)[current_tag_idx])\n\u001b[0;32m     42\u001b[0m     current_tag_idx \u001b[38;5;241m=\u001b[39m backpointer[i, current_tag_idx]\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m most_probable_tags\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "jumbled_sentence = \"The quick brown fox over jumps the lazy dog\"\n",
    "most_probable_tags = find_tag_sequence(jumbled_sentence)\n",
    "print(f\"Most probable tags for the jumbled sentence: {', '.join(most_probable_tags)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2754c2a2-cd61-4537-929f-f3185175ef4a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
